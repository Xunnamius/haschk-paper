\section{Introduction} \label{sec:introduction}

In 2010, through compromising legitimate applications available on trusted
vendor websites, nation-state actors launched the Havex malware, targeting
aviation, defense, pharmaceutical, and other companies in Europe and the United
States~\cite{SCA-HAVEX1, SCA-HAVEX2}. In 2012, attackers compromised an official
phpMyAdmin download mirror hosted by reputable software provider SourceForge.
The backdoored version of the popular database frontend was downloaded by
hundreds of users, potentially allowing attackers to gain access to private
customer data~\cite{SCA-PMA1, SCA-PMA2}. In 2016, attackers broke into the Linux
Mint distribution server and replaced a legitimate Mint ISO with one containing
a backdoor, infecting hundreds of machines with the malware~\cite{SCA-MINT1,
SCA-MINT2}. Over a four day period in 2017, users of the popular HandBrake open
source video transcoder on Mac/OSX were made aware that, along with their
expected video software, they may have also downloaded a trojan that was
uploading their sensitive data to a remote server~\cite{SCA-HB1}. HandBrake
developers recommended users perform \emph{checksum verification} to determine
if their install was compromised~\cite{SCA-HB2}.

Clearly, downloading resources over the internet comes with considerable risk.
This risk can be divided into three broad concerns: response authentication,
communication confidentiality, and resource integrity. Response authentication
allows us to determine if a response received indeed originates from its
purported source through, for instance, the adoption of a Public Key
Infrastructure (PKI) scheme~\cite{PKI}. Communication confidentiality allows us
to keep the data transacted between two or more parties private through some
form of encryption, such as AES~\cite{AES}. Finally, resource integrity allows
us to verify that the data we are receiving is the data we are expecting to
receive.

When it comes to response authentication and communication confidentiality
concerns on the internet, the state-of-the-art in attack mitigation is Transport
Layer Security (TLS) and its Hyper Text Transfer Protocol (HTTP)/PKI based
implementation, HTTPS~\cite{TLS1.2, TLS1, TLS0, HTTPS, PKI}. Assuming well
behaved certificate authorities and modern browsing software, TLS and related
protocols, when properly deployed, mitigate myriad attacks on authentication and
confidentiality.

However, as a \textit{communication} protocol, TLS only guarantees the integrity
of each \textit{end-to-end communication} via message authentication code
(MAC)~\cite{TLS1.2}. But protected encrypted communications mean nothing if the
contents of those communications are corrupted before the fact. Hence, attacks
against the integrity of resources at the application layer (rather than the
transport layer) are outside the threat model addressed by TLS and
HTTPS~\cite{TLS1.2, HTTPS}.

Attacks on resource integrity can be considered a subset of \emph{Supply Chain
Attacks} (SCA). Rather than attack an entity directly, SCAs are the compromise
of an entity's software source code (or any product) via cyber attack, insider
threat, upstream asset compromise, trusted hardware/vendor compromise, or other
attack on one or more phases of the software development life
cycle~\cite{NIST-SCA}. These attacks are hard to detect, even harder to prevent,
and have the goal of infecting and exploiting targets and victims by abusing the
trust between consumer and reputable software vendor~\cite{SCA}.

Ensuring the integrity of resources exchanged over the internet despite SCAs and
other active attacks is a hard and well studied problem~\cite{MD5Header,
HTTP1.1, HTTPS, SRI, LF, OpenPGP1, DNSSEC, PKI, Cherubini, Stickler}. For a long
time, the de facto standard for addressing this risk in the generic case has
been the use of \textit{checksums} coupled with some secure transport medium
like TLS/HTTPS. Checksums in this context are cryptographic digests generated by
a cryptographic hashing function~\cite{Rogaway} run over the resource's file
contents. When a user downloads a file from some source, they are expected to
run the same cryptographic hashing function over their version of the resource
to yield a local checksum and then match it with a checksum given to them by
some trusted authority.

However, checksums come up short as a solution to the resource integrity
problem. Foremost is a well-understood but harsh reality: \textbf{user apathy}.
Most users will not be inconvenienced with manually calculating checksums for
the resources they download~\cite{Cherubini, Fagan}; moreover, most users will
not take the time to understand how checksums and integrity verification
work~\cite{Cherubini, Tan, Hsiao}. While detailing how they gained unauthorized
access to the servers, one of the hackers behind the 2016 breach of Linux Mint's
distribution system went so far as to comment (in respect to checksums): ``Who
the [expletive] checks those anyway?''~\cite{SCA-MINT3}. Hardly unique to
checksums, designers of security schemes from HTTPS to PGP have found user
apathy a difficult problem space to navigate~\cite{PGPBad, Cherubini}. When it
comes to user apathy versus security warnings (\eg{malicious download alerts,
warnings about compromised websites, expired certificate notifications}),
application developers and user interface (UI) designers face similar
difficulties~\cite{Egelman1, Egelman2, Modic, Reeder, Silic, Sunshine, Bianchi,
Akhawe}.

Even if a user feels the urge to manually calculate and verify a resource's
checksum, they must search for a corresponding ``authoritative checksum'' to
compare against. As there is no standard storage or retrieval methods for
checksums, they could be stored anywhere, or even in multiple different
locations that must then be kept consistent~\cite{Cherubini}; users are not
guaranteed to find an authoritative checksum, even if they are published online
somewhere, even if they appear on the same page as the resource they are trying
to protect. If users do manage to find the authoritative checksum manually and
also recognize the checksums are different, the user is then expected to ``do
the right thing,'' whatever that happens to be in context.

A major contributor to this confusion is the tradeoff made by
\textbf{co-hosting} a resource and its authoritative checksum on the same
distribution system (\eg{a web page from a server}). While cost-effective for
the developer and less confusing for the user, an attacker that compromises a
system hosting both a resource and its checksum together can mutate both,
rendering the checksum irrelevant~\cite{Stickler}. This is true for automated
checksum verification solutions as well~\cite{Cherubini}. The co-hosting problem
was demonstrated by the 2016 hack of Linux Mint's distribution
server~\cite{SCA-MINT1, SCA-MINT2}.

For these reasons, checksums as they are typically deployed are not very
effective at guaranteeing resource integrity, even if automatic verification by
web clients is attempted. Recognizing this, some corporations and large entities
rely instead on approaches like digital signature validation, code signing, and
Binary Transparency~\cite{PKI, BinaryTransparency}. These roll-your-own
solutions, often proprietary, have been deployed successfully to mitigate
resource integrity attacks in mature software package ecosystems like Debian/apt
and Red Hat/yum and walled-garden app stores like Google Play, Apple App Store,
and the Microsoft Store.

Unfortunately, not all resources available on the internet are acquired through
mature software package ecosystems with built-in PKI support. In the United
States for instance, most internet users download software directly from
websites or other locations~\cite{Cherubini, Ryan}. Nor are all resources
downloaded over the internet software binaries. Moreover, such schemes are not
compatible with one another and cannot scale to secure arbitrary resources on
the internet without significant cost and implementation/deployment effort.

In this paper we address these problems by proposing \SYSTEM{}, a novel protocol
for verifying the integrity of arbitrary resources downloaded over the internet
that is a complete replacement for typical checksum-based schemes, significantly
raises the bar for the attacker, and can be implemented in more than just
browser software. To overcome the challenges posed by user apathy and
co-hosting, \SYSTEM{} is implemented in two parts: a backend for resource
\emph{providers} and a frontend for resource \emph{consumers}---i.e. end users.
Providers use a high availability backend to advertise which resources they
provide for download. To defeat co-hosting, this backend must exist separately
from the system offering those resources. To account for user apathy, the
frontend client must automatically compute a (non-authoritative) checksum
identifying the resource, query the backend using that checksum, and mitigate
the threat to the user in the case where the download is deemed compromised.
Hence, \SYSTEM{} consists of both the implementation of these parts and the
protocol by which these parts communicate.

We approach the problem with four key concerns in mind. (1) \SYSTEM{} frontends
must provide security guarantees transparently without adding any extra burden
on the user in the common case. Here, an optimal implementation avoids relying
on the user to overcome apathy in the interest of security while accounting for
users' tendency to click through security warnings to avoid inconvenience. (2)
\SYSTEM{} must be low effort for developers to integrate and deploy in concert
with the resource(s) they are meant to protect. There must be no requirement to
configure a secondary system solely dedicated to hosting checksums. (3) The
protocol is not tightly coupled with any particular high availability system.
(4) No application or website source code changes or user-facing server or web
infrastructure modifications are necessary.

To demonstrate the general applicability of our protocol, we implement a
frontend Google Chrome extension and two different high availability backends:
one based on the public Domain Name System (DNS) via Google DNS and another
based on the Ring OpenDHT network via a local Representational State Transfer
(REST) API. Of course, these \SYSTEM{} component implementations should be
considered proof-of-concept.

We evaluate the security, performance, scalability, and deployment overhead of
our implementation. While not a panacea, we find no practical obstacles to
efficient deployment at scale or to security outside of those imposed by the
Chrome developer API.

In summary, our primary contributions are: \\

1. We propose a practical automated approach to transparently mitigating the
accidental consumption of compromised resources over the internet. Our protocol
requires no modifications to web standards, web infrastructure, or source code,
does not employ unreliable heuristics, does not expect checksums to be co-hosted
on the same page as do other automated approaches, and can be transparently
implemented and immediately deployed. \SYSTEM{}-aware clients will have their
downloads secured while the user experience of clients unaware of \SYSTEM{}
remain completely unaffected. Further, our protocol can be implemented in more
than just browsers; \SYSTEM{} can be implemented to protect downloads in FTP
clients, SSH (like rsync) clients, etc. \\

2. We present our proof-of-concept implementation, a Google Chrome extension,
and demonstrate our protocol's effectiveness empirically using a publicly
available patched HotCRP instance. We show that our implementation is capable of
detecting real-world resource compromises in this instance, even when the entire
server is compromised, significantly raising the bar for an attacker. We
additionally make available our frontend implementation and OpenDHT backend open
source (see \secref{availability}). \\

3. We evaluate our implementation and find no practical obstacles to efficient
deployment outside of those imposed by the Chrome developer API. To the best of
our knowledge, this is the first approach that is not susceptible to the
pitfalls of co-hosting. We conclude that \SYSTEM{} is more effective at
detecting resource integrity attacks than manual checksum verification and prior
automated verification schemes. Further, we provide this capability with
marginal deployment overhead for providers.
