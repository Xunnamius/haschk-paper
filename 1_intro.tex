\section{Introduction} \label{sec:introduction}

In 2010, through compromising legitimate applications available on trusted
vendor websites, nation-state actors launched the Havex malware, targeting
aviation, defense, pharmaceutical, and other companies in Europe and the United
States~\cite{SCA-HAVEX1, SCA-HAVEX2}. In 2012, attackers compromised an official
phpMyAdmin download mirror hosted by reputable software provider SourceForge.
The backdoored version of the popular database frontend was downloaded by
hundreds of users, potentially allowing attackers to gain access to private
customer data~\cite{SCA-PMA1, SCA-PMA2}. In 2016, attackers broke into the Linux
Mint distribution server and replaced a legitimate Mint ISO with one containing
a backdoor, infecting hundreds of machines with the malware~\cite{SCA-MINT1,
SCA-MINT2}. Over a four day period in 2017, users of the popular HandBrake open
source video transcoder on Mac/OSX were made aware that, along with their
expected video software, they may have also downloaded a trojan that was
uploading their sensitive data to a remote server~\cite{SCA-HB1}. HandBrake
developers recommended users perform \emph{checksum verification} to determine
if their install was compromised~\cite{SCA-HB2}.

Clearly, downloading resources over the internet comes with considerable risk.
This risk can be divided into three broad concerns: response authentication,
communication confidentiality, and resource integrity. Response authentication
allows us to determine if a response received indeed originates from its
purported source through, for instance, the adoption of a Public Key
Infrastructure (PKI) scheme~\cite{PKI}. Communication confidentiality allows us
to keep the data transacted between two or more parties private through some
form of encryption, such as AES~\cite{AES}. Finally, resource integrity allows
us to verify that the data we are receiving is the data we are expecting to
receive.

When it comes to response authentication and communication confidentiality
concerns on the internet, the state-of-the-art in attack mitigation is Transport
Layer Security (TLS) and its Hyper Text Transfer Protocol (HTTP)/PKI based
implementation, HTTPS~\cite{TLS1.2, TLS1, TLS0, HTTPS, PKI}. Assuming well
behaved certificate authorities and modern browsing software, TLS and related
protocols, when properly deployed, mitigate myriad attacks on authentication and
confidentiality.

However, as a \textit{communication} protocol, TLS only guarantees the integrity
of each \textit{end-to-end communication} via message authentication code
(MAC)~\cite{TLS1.2}. But protected encrypted communications mean nothing if the
contents of those communications are corrupted before the fact. Hence, attacks
against the integrity of resources at the application layer (rather than the
transport layer) are outside the threat model addressed by TLS and
HTTPS~\cite{TLS1.2, HTTPS}.

Attacks on resource integrity can be considered a subset of \emph{Supply Chain
Attacks} (SCA). Rather than attack an entity directly, SCAs are the compromise
of an entity's software source code (or any product) via cyber attack, insider
threat, upstream asset compromise, trusted hardware/vendor compromise, or other
attack on one or more phases of the software development life
cycle~\cite{NIST-SCA}. These attacks are hard to detect, even harder to prevent,
and have the goal of infecting and exploiting targets and victims by abusing the
trust between consumer and reputable software vendor~\cite{SCA}.

Ensuring the integrity of resources exchanged over the internet despite SCAs and
other active attacks is a hard and well studied problem~\cite{MD5Header,
HTTP1.1, HTTPS, SRI, LF, OpenPGP1, DNSSEC, PKI, Cherubini, Stickler}. For a long
time, the de facto standard for addressing this risk in the generic case has
been the use of \textit{checksums} coupled with some secure transport medium
like TLS/HTTPS. Checksums in this context are cryptographic digests generated by
a cryptographic hashing function~\cite{Rogaway} run over the resource's file
contents. When a user downloads a file from some source, they are expected to
run the same cryptographic hashing function over their version of the resource
to yield a local checksum and then match it with a checksum given to them by
some trusted authority.

However, checksums come up short as a solution to the resource integrity
problem. Foremost is a well-understood but harsh reality: \textbf{user apathy}.
Most users will not be inconvenienced with manually calculating checksums for
the resources they download~\cite{Cherubini, Fagan}; moreover, most users will
not take the time to understand how checksums and integrity verification
work~\cite{Cherubini, Tan, Hsiao}. While detailing how they gained unauthorized
access to the servers, one of the hackers behind the 2016 breach of Linux Mint's
distribution system went so far as to comment (in respect to checksums): ``Who
the [expletive] checks those anyway?''~\cite{SCA-MINT3}. Hardly unique to
checksum calculation, designers of security schemes from HTTPS to PGP have found
user apathy a difficult problem space to navigate~\cite{PGPBad, Cherubini}. When
it comes to user apathy versus security warnings (\eg{malicious download alerts,
warnings about compromised websites, expired certificate notifications}),
application developers and user interface (UI) designers face similar
difficulties~\cite{Egelman1, Egelman2, Modic, Reeder, Silic, Sunshine, Bianchi,
Akhawe}.

Even if a user felt the urge to manually calculate and verify a resource's
checksum, they must search for a corresponding ``authoritative checksum'' to
compare against. As there is no standard storage or retrieval methods for
checksums, they could be stored anywhere, or even in multiple different
locations that must then be kept consistent~\cite{Cherubini}; users are not
guaranteed to find an authoritative checksum, even if they are published online
somewhere, even if they appear on the same page as the resource they are trying
to protect. If users do manage to find the authoritative checksum manually and
also recognize the checksums are different, the user is then expected to ``do
the right thing,'' whatever that happens to be in context.

A major contributor to this confusion is the tradeoff made by
\textbf{co-hosting} a resource and its authoritative checksum on the same
distribution system or web page. While cost-effective for the developer and less
confusing for the user, an attacker that compromises a system hosting a resource
and its checksum together can mutate both, rendering the checksum
irrelevant~\cite{Stickler}. This is true for automated checksum verification
solutions as well~\cite{Cherubini}. The co-hosting problem was demonstrated by
the 2016 hack of Linux Mint's distribution server~\cite{SCA-MINT1, SCA-MINT2}.

For these reasons, checksums as they are typically deployed are not very
effective at guaranteeing resource integrity, even if automatic verification by
web clients is attempted. Recognizing this, some corporations and large entities
rely instead on approaches like digital signature validation, code signing, and
Binary Transparency~\cite{PKI, BinaryTransparency}. These roll-your-own
solutions, often proprietary, have been deployed successfully to mitigate
resource integrity attacks in mature software package ecosystems like Debian/apt
and Red Hat/yum and walled-garden app stores like Google Play, Apple App Store,
and the Microsoft Store.

Unfortunately, not all resources available on the internet are acquired through
mature software package ecosystems with built-in PKI support. In the United
States for instance, most internet users download software directly from
websites or other locations~\cite{Cherubini, Ryan}. Nor are all resources
downloaded over the internet software binaries. Moreover, such schemes are not
compatible with one another and cannot scale to secure arbitrary resources on
the internet without significant cost and implementation/deployment effort.


\TODO{Transition here is a little abrupt.  You cover a lot of ground in the
previous paragraphs, so you need a tranisition clause that brings it all
together.  Something like this paper addresses the challenges that user apathy
and co-hosting present to the problem of resource integrity protection by
proposing \SYSTEM{}, a novel...} In this paper, we propose \SYSTEM{}, a novel
approach for verifying the integrity of resources downloaded over the internet
that is a complete replacement for typical checksum-based schemes and
significantly raises the bar for the attacker.

We view the problem with four key concerns in mind. 1) \SYSTEM{} implementations
must provide security guarantees transparently without adding any extra burden
on the user in the common case. Here, an optimal implementation avoids relying
on the user to overcome apathy in the interest of security while accounting for
users' tendency to click through security warnings. 2) \SYSTEM{} must be low
effort for developers to integrate and deploy in concert with the resource(s) it
is meant to protect. There must be no requirement to configure a secondary
system solely dedicated to hosting checksums. 3) The verification method is not
tightly coupled with any particular high availability system. 4) No application
or website source code changes or user-facing server or web infrastructure
modifications are necessary.

\TODO{Added 9:18 PM 1/9/2020: I realized as I was thinking over this paper, that it never explicitly says what \SYSTEM{} is.  Is it a framework? A protocol? A software system?  I believe it is both a protocol and two implementations of that protocol, but the introduction needs a paragraph that explicitly describes what the proposed thing actually is and what is novel about it.  I realize that the paragraph immediately above sort of does that, but it is too oblique, the paper needs to explicitly state what the proposal is.} 

\TODO{NEed another transition here.  Why two implementations? That needs to be
spun positively by saying that the idea generalizes in some respect, so it is
not tied to some other technology.} We implement \SYSTEM{} as two
proof-of-concept Google Chrome extensions: one relying on DNS as a high
availability backend and the other based on an OpenDHT-based high availability
Ring backend.




We then evaluate the security, deployability, scalability, and performance of
our implementations. Specifically, we find no practical obstacles to efficient
deployment at scale outside of those imposed by the client and the chosen high
availability system. Additionally, we provide a publicly accessible empirical
demonstration of \SYSTEM{}'s utility via a patched HotCRP instance. We also make
available our \DNSSYS{} and \DHTSYS{} implementations to the community as open
source software to promote exploration of the \SYSTEM{} approach (cf.
\secref{availability}).

In summary, our primary contributions are: \\

1) A practical automated approach to transparently mitigating the accidental
consumption of compromised resources over the internet. Our approach requires no
modifications to web standards, web infrastructure, or source code, does not
employ unreliable heuristics, does not expect checksums to be co-hosted on the
same page as do other automated approaches, and can be transparently implemented
and immediately deployed. \SYSTEM{}-aware clients will have their downloads
secured while the user experience of clients unaware of \SYSTEM{} remain
completely unaffected. Further, our approach can be applied to more than just
browsers; \SYSTEM{} can be implemented to protect downloads in FTP clients, SSH
(like rsync) clients, etc. \\

2) We present our proof-of-concept \SYSTEM{} implementations (Google Chrome
extensions): \DNSSYS{} and \DHTSYS{}. We demonstrate our implementations'
effectiveness empirically using a publicly available (patched) HotCRP instance.
We show that our \DNSSYS{} implementation is capable of detecting real-world
resource compromises in a patched HotCRP instance, even when the entire server
is compromised and/or the user is redirected to a hostile server, significantly
raising the bar for an attacker. \\

3) We evaluate the security, performance, scalability, and deployment overhead
of \DNSSYS{} and \DHTSYS{}. We find that our approach is more effective than
manual checksum verification or prior automated verification schemes at
detecting resource integrity attacks. To the best of our knowledge, this is the
first approach that 1) is not susceptible to the pitfalls of co-hosting or the
lack of standard practices for making authoritative checksums available to end
users, unlike a manual verification approach or other automated solutions and 2)
provides such capabilities with marginal deployment overhead for developers.

