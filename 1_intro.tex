\section{Introduction} \label{sec:introduction}

In 2010, through compromising legitimate applications available on trusted
vendor websites, nation-state actors launched the Havex malware, targeting
aviation, defense, pharmaceutical, and other companies in Europe and the United
States~\cite{SCA-HAVEX1, SCA-HAVEX2}. In 2012, attackers compromised an official
phpMyAdmin download mirror hosted by reputable software provider SourceForge.
The backdoored version of the popular database frontend was downloaded by
hundreds of users, potentially allowing attackers to gain access to private
customer data~\cite{SCA-PMA1, SCA-PMA2}. In 2016, attackers broke into the Linux
Mint distribution server and replaced a legitimate Mint ISO with one containing
a backdoor, infecting hundreds of machines with the malware~\cite{SCA-MINT1,
SCA-MINT2}. Over a four day period in 2017, users of the popular HandBrake open
source video transcoder on Mac/OSX were made aware that, along with their
expected video software, they may have also downloaded a trojan that was
uploading their sensitive data to a remote server~\cite{SCA-HB1}. HandBrake
developers recommended users perform \emph{checksum verification} to determine
if their install was compromised~\cite{SCA-HB2}.

Clearly, downloading resources over the internet (TCP/IP) comes with
considerable risk. This risk can be divided into three broad concerns: response
authentication, communication confidentiality, and resource integrity. Response
authentication allows us to determine if a response received indeed originates
from its purported source through, for instance, the adoption of a Public Key
Infrastructure (PKI) scheme~\cite{PKI}. Communication confidentiality allows us
to keep the data transacted between two or more parties private through some
form of encryption, such as AES~\cite{AES}. Finally, resource integrity allows
us to verify that the data we are receiving is the data we are expecting to
receive.

When it comes to response authentication and communication confidentiality
concerns on the internet, the state-of-the-art in attack mitigation is Transport
Layer Security (TLS) and its Hyper Text Transfer Protocol (HTTP)/PKI based
implementation, HTTPS~\cite{TLS1.2, TLS1, TLS0, HTTPS, PKI}. Assuming well
behaved certificate authorities and modern browsing software, TLS and related
protocols, when properly deployed, mitigate myriad attacks on authentication and
confidentiality.

However, as a \textit{communication} protocol, TLS only guarantees the integrity
of each \textit{end-to-end communication} via message authentication code
(MAC)~\cite{TLS1.2}. But protected encrypted communications mean nothing if the
contents of those communications are corrupted before the fact. Hence, attacks
against the integrity of resources at the application layer (rather than the
transport layer) are outside the threat model addressed by TLS and
HTTPS~\cite{TLS1.2, HTTPS}.

Attacks on resource integrity can be considered a subset of \emph{Supply Chain
Attacks} (SCA). Rather than attack an entity directly, SCAs are the compromise
of an entity's software source code (or any product) via cyber attack, insider
threat, upstream asset compromise, trusted hardware/vendor compromise, or other
attack on one or more phases of the software development life
cycle~\cite{NIST-SCA}. These attacks are hard to detect, even harder to prevent,
and have the goal of infecting and exploiting targets and victims by abusing the
trust between consumer and reputable software vendor~\cite{SCA}.

Ensuring the integrity of resources exchanged over the internet despite SCAs and
other active attacks is a hard and well studied problem~\cite{MD5Header,
HTTP1.1, HTTPS, SRI, LF, OpenPGP1, DNSSEC, PKI, Cherubini, Stickler}. The de
facto standard for addressing this risk in the generic case is using
\textit{checksums} coupled with some secure transport medium like TLS/HTTPS.
Checksums in this context are cryptographic digests generated by a cryptographic
hashing function~\cite{Rogaway} run over the resource's file contents. When a
user downloads a file from some source, they are expected to run the same
cryptographic hashing function over their version of the resource to yield a
local checksum and then match it with a checksum given to them by some trusted
authority.

However, checksums come up short as a solution to the resource integrity
problem. Foremost is a well-understood but harsh reality: \textbf{user apathy}.
Most users will not be inconvenienced with manually calculating checksums for
the resources they download~\cite{Cherubini, Fagan}; moreover, most users will
not take the time to understand how checksums and integrity verification
work~\cite{Cherubini, Tan, Hsiao}. While detailing how they gained unauthorized
access to the servers, one of the hackers behind the 2016 breach of Linux Mint's
distribution system went so far as to comment (in respect to checksums): ``Who
the [expletive] checks those anyway?''~\cite{SCA-MINT3}. Hardly unique to
checksums, designers of security schemes from HTTPS to PGP to Google Chrome
dangerous download warnings have found user apathy a difficult problem space to
navigate~\cite{PGPBad, Cherubini, Akhawe, ChromeClickThrough, Egelman1,
Egelman2, Modic, Reeder, Silic, Bianchi}.

Even if a user feels the urge to manually calculate and verify a resource's
checksum, they must search for a corresponding ``authoritative checksum'' to
compare against. As there is no standard storage or retrieval methods for
checksums, they could be stored anywhere, or even in multiple different
locations that must then be kept consistent~\cite{Cherubini}; users are not
guaranteed to find an authoritative checksum, even if they are published online
somewhere, even if they appear on the same page as the resource they are trying
to protect. If users do manage to find the authoritative checksum manually and
also recognize the checksums are different, the user is then expected to ``do
the right thing,'' whatever that happens to be in context.

A major contributor to this confusion is the tradeoff made by
\textbf{co-hosting} a resource and its authoritative checksum on the same
distribution system, \eg{ a web page hosting both a hyperlink pointing to a
resource and that resource's authoritative checksum together}. While
cost-effective for the provider and less confusing for the user, an attacker
that compromises a system hosting both a resource and its checksum together can
mutate both, rendering the checksum irrelevant~\cite{Stickler}. This is true for
automated checksum verification solutions as well~\cite{Cherubini}. The
co-hosting problem was demonstrated by the 2016 hack of Linux Mint's
distribution server~\cite{SCA-MINT1, SCA-MINT2}.

For these reasons, checksums as they are typically deployed are not very
effective at guaranteeing resource integrity, even if automatic verification by
web clients is attempted. Recognizing this, some corporations and large entities
rely instead on approaches like digital signature verification, code signing,
and Binary Transparency~\cite{PKI, BinaryTransparency}. These roll-your-own
solutions, often proprietary, have been deployed successfully to mitigate
resource integrity attacks in mature software package ecosystems like Debian/apt
and Red Hat/yum and walled-garden app stores like Google Play, Apple App Store,
and the Microsoft Store.

Unfortunately, not all resources available on the internet are acquired through
mature software package ecosystems with built-in PKI support. In the United
States for instance, most internet users download software directly from
websites or other locations~\cite{Cherubini, Ryan}. 
%Nor are all resources downloaded over the internet software binaries. 
Moreover, such schemes are not compatible with one another and cannot scale to
secure arbitrary resources on the internet without significant cost and
implementation/deployment effort.

This paper addresses these problems by proposing \SYSTEM{}, a novel protocol for
verifying the integrity of arbitrary resources downloaded over the internet that
is a complete replacement for typical checksum-based schemes, significantly
raises the bar for the attacker, and can be implemented in more than just
browser software. To overcome the challenges posed by user apathy and
co-hosting, \SYSTEM{} is implemented in two parts: a backend for resource
\emph{providers} and a frontend for resource \emph{consumers}---\ie{ (end)
users}. Providers use a high availability backend to advertise which resources
they provide for download. To defeat co-hosting, this backend exists separately
from the system offering those resources. To account for user apathy, the
frontend client automatically computes a (non-authoritative) checksum
identifying the resource, queries the backend using that checksum, and mitigates
the threat to the user in the case where the download is deemed compromised.
Hence, \SYSTEM{} consists of both the protocol by which these pieces communicate
and their implementation.

We approach these problems with four key concerns in mind. (1) \SYSTEM{}
frontends must provide security guarantees transparently without adding any
extra user burden in the common case. Here, an optimal implementation avoids
relying on the user to overcome apathy in the interest of security while
accounting for the tendency of some users to click through security warnings so
as to not be inconvenienced. The former is achieved through automation and the
latter through careful design. (2) \SYSTEM{} must be low effort for providers to
integrate and deploy in concert with the resource(s) they are meant to protect.
There must be no requirement to configure a secondary system solely dedicated to
hosting checksums. (3) The protocol is not tightly coupled with any particular
high availability system. (4) Neither application, website source code changes,
user-facing server, nor web infrastructure modifications are necessary for
providers to deploy \SYSTEM{}.

To demonstrate the general applicability of our protocol, we implement a
frontend Google Chrome extension and two different high availability backends:
one based on the public Domain Name System (DNS)~\cite{DNS1, DNS2} via Google
DNS~\cite{GoogleDNS} and another based on the Ring OpenDHT
network~\cite{OpenDHT, savoirfairelinux} via a custom Representational State
Transfer (REST) API. Of course, these \SYSTEM{} components of our implementation
should be considered proof-of-concept.

We evaluate the security, performance impact, scalability, and deployment
overhead of our implementation using a publicly available patched HotCRP
instance. While not a panacea, we show that our implementation is capable of
detecting real-world resource compromises, even when the server is under the
attacker's control. Additionally, we find no practical obstacles to efficient
deployment at scale or to security outside of those imposed by the Chrome V2
Extension API.

In summary, our primary contributions are:

First, we propose a practical automated approach to ensuring the integrity of
arbitrary downloads. Our protocol requires no modifications to web
standards/infrastructure or source code, does not employ unreliable heuristics,
does not expect checksums to be co-hosted on the same page as do other automated
approaches, and can be transparently implemented. Further, our protocol can be
used by more than just browsers; \SYSTEM{} can protect downloads in FTP clients,
wget/curl clients, and other software.

Second, we present our proof-of-concept implementation, a Google Chrome
extension, and demonstrate our protocol's effectiveness empirically. We show
that our implementation is capable of detecting resource compromises when the
server is under the attacker's control, thus significantly raising the bar for
an attacker. We additionally make our frontend implementation and OpenDHT JSON
API available open source (see \secref{availability}).

Third, we evaluate our implementation and find marginal integration and
deployment overhead with no practical obstacles to scalability or security
outside those imposed by the Chrome API. To the best of our knowledge, this is
the first approach that is not susceptible to the pitfalls of co-hosting. Hence,
we conclude that \SYSTEM{} is more effective at detecting resource integrity
attacks than manual checksum verification and prior automated schemes.
