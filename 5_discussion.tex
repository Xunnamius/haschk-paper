\section{Discussion} \label{sec:discussion}

In this section, we examine current and previous DNS-based and other approaches
to problems related to \SYSTEM{}. We specifically note PGP's limiting human
factors, how those factors similarly apply to the application of checksums for
resource integrity validation, and how our \DNSSYS{} implementation avoids these
factors. Thereafter, we discuss some limitations of our implementations.

\subsection{Additional Related Work}

\noindent\textbf{Cryptographic Data in DNS Resource Records.}

The DNS-Based Authentication of Named Entities (DANE) specification~\cite{DANE1,
DANE2, DANE3} defines the ``TLSA'' and ``OPENPGPKEY'' DNS resource records to
store cryptographic data. These resource record types, along with
``CERT''~\cite{CERT}, ``IPSECKEY''~\cite{IPSECKEY}, those defined by DNS
Security Extensions (DNSSEC)~\cite{DNSSEC}, and others demonstrate that storing
useful cryptographic data retrievable through the DNS network is feasible at
scale. Due the unique requirements of \DNSSYS{}, however, we use ``TXT'' records
to map Resource Identifiers to Authoritative Checksums. In accordance with RFC
5507~\cite{RFC5507}, a production \DNSSYS{} implementation would necessitate the
creation of a new DNS resource record type as no current resource record type
meets the requirements of \DNSSYS{}. \\

\noindent\textbf{PGP/OpenPGP.} Though PGP addresses a fundamentally different
authentication-focused threat model compared with \SYSTEM{}, it is useful to
note: many of the same human and UX factors that make the cryptographically
solid OpenPGP standard and its various implementations so unpleasant for end
users also exist in the context of download integrity verification and
checksums. End users cannot and \textit{will not} be burdened with manually
verifying a checksum; as was the case with PGP 5.0~\cite{PGPBad}, some users are
likely confused by the very notion and function of a checksum, if they are aware
that checksums exist at all. If PGP's adoption issues are any indication, users
of a security solution that significantly complicates an otherwise simple task
are more likely to bypass said solution rather than be burdened with it. To
assume otherwise can have disastrous consequences~\cite{PGPBad} (cf.
\secref{background}). \\

\noindent\textbf{Link Fingerprints and Subresource Integrity.} The Link
Fingerprints (LF) draft describes an early HTML anchor and URL based resource
integrity verification scheme~\cite{LF}. Subresource Integrity (SRI) describes a
similar production-ready HTML-based scheme designed with CDNs and web assets
(rather than generic resources) in mind. Like \SYSTEM{}, both LF and SRI employ
cryptographic digests to ensure no changes of any kind have been made to a
resource~\cite{SRI}. Unlike \SYSTEM{}, LF and SRI rely on the server that hosts
the HTML source to be secure; specifically, the checksums contained in the HTML
source must be accurate for these schemes to work. An attacker that has control
of the web server can alter the HTML and inject a malicious checksum. With
\SYSTEM{}, however, an attacker would additionally have to compromise whichever
authenticated distributed system hosted the mappings between Resource
Identifiers and Authoritative Checksums. \\

\noindent\textbf{Content-MD5 Header.} The Content-MD5 header field is a
deprecated email and HTTP header that delivers a checksum similar to those used
by Subresource Integrity. It was removed from the HTTP/1.1 specification because
of the inconsistent implementation of partial response handling between
vendors~\cite{HTTP1.1}. Further, the header could be easily stripped off or
modified by proxies and other intermediaries~\cite{MD5Header}. \\

\noindent\textbf{Reproducible Builds/Deterministic Build Systems.} A
deterministic build system is one that, when given the same source, will
deterministically output the same binary on every run. For example, many
packages in Debian~\cite{ReproBuildsDebian} and Arch Linux can be rebuilt from
source, yielding an identical byte for byte result each time via a reproducible
build process~\cite{ReproBuilds}. When a deterministic build system is coupled
with the \SYSTEM{} approach, a chain of trust can be established that links the
\emph{Development} and \emph{Integration} supply chain phases to the
\emph{Deployment}, \emph{Maintenance}, and \emph{Retirement} supply chain phases
(cf. \tblref{attacks}), further raising the bar for the attacker.

\subsection{Implementation-specific Limitations}

\subsubsection{DNSSEC Adoption is Slow}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{apnic.png}
    \caption{APNIC estimate of the percentage of global DNS resolvers (Google
    PDNS as well as local resolvers) performing DNSSEC validation from October
    2013 to December 2018. The five year trend is positive.}\label{fig:apnic}
\end{figure*}

The proof-of-concept \DNSSYS{} implementation is only secure if the
corresponding DNS zone is secure, \ie it is protected by DNSSEC. As detailed in
\figref{apnic}, DNSSEC adoption rate---which has increased dramatically since
the first production root zone was signed in 2010~\cite{Cloudflare, APNIC}---is
decidedly variable and slow to rise. Only around 3\% of Fortune 1000 and 9\% of
university domains have properly deployed DNSSEC~\cite{NIST-IPv6}, and the
number of DNS resolvers validating DNSSEC replies currently sits at
approximately 12-14\%~\cite{APNIC}.

While this could be happening for a variety of reasons~\cite{DNSSEC-is-hard-1,
DNSSEC-is-hard-2, DNSSEC-is-hard-3, DNSSEC-is-hard-4, DNSSEC-is-hard-5}, slow
growth is certainly not outside of the norm for global protocol deployments that
are perceived as ``nice to have'' rather than ``business critical''. For
instance: the adoption rate of IPv6, proposed nearly 25 years ago, is similarly
slow to rise. Globally, when measured as the availability of IPv6 connectivity
among users accessing any Google service, it rests at approximately
21\%~\cite{Google-IPv6}, with only 2\% of Fortune 1000 and 3\% of university
domains being IPv6-enabled~\cite{NIST-IPv6}; we note that this is the case
despite IANA and all RIRs having entered the final stage of virtual IPv4
address space exhaustion as of 2018~\cite{APNIC-exhaustion} while the number of
internet-connected devices continues its upward climb~\cite{Cisco}.

With that said, if we assume the user has installed the \DNSSYS{} extension,
slow adoption of DNSSEC globally would in no way impact an individual entity's
ability to adopt and immediately benefit from \DNSSYS{}. We consider this a key
feature of the approach. Those entities that consider their resources' integrity
to be business critical do not have to wait for DNSSEC to be adopted globally.
Any well-configured DNSSEC-protected zone can opt-in to providing the resource
records \DNSSYS{} expects, including a strict mode record, offering power users
verifiable resources while remaining completely transparent to everyone else.
Otherwise, users receiving resources from an entity that is not yet DNSSEC
capable (and so they do not support \DNSSYS{}) will not experience any
interruption in their user experience whether they have a \DNSSYS{}-capable
browser or not.

\subsubsection{DNS-Specific Protocol Limitations}

Clearly, \DNSSYS{} relies on DNS. However, DNS~\cite{DNS1} was not originally
designed to transport or store relatively large amounts of data, though this has
been addressed with EDNS0~\cite{EDNS}. The checksums stored in DNS should not be
much longer than 128 bytes or the output of the SHA512 function. Regardless, DNS
resource record extensions exist that store much more than 128 bytes of
data~\cite{CERT, IPSECKEY, DANE3, DANE1}.

Several working groups are considering DNS as a storage medium for
checksums/hash output as well, such as securitytxt~\cite{draft-sectxt}. A widely
deployed example of DNS ``TXT'' resource records being used this way is SPF and
DKIM~\cite{DKIM}. We are unaware of any practical limitation on the number of
resource records a DNS zone file can support~\cite{DNS1}, hence any
considerations regarding zone file size and/or ceilings on the number of TXT
records in a single zone are at the sole discretion of the implementing entity.

Additionally, \DNSSYS{} does not add to the danger of amplification and other
reflection attacks on DNS; these are generic DNS issues addressable at other
layers of the protocol.

\subsection{DHT-Specific Limitations}

Unlike DNS, an entity seeking to leverage a Distributed Hash Table (DHT) may not
have the benefit of being able to rely on a high availability distributed
authenticated backend that is already established, is well-tested, and exists
globally like DNS. Such an entity would have to either maintain their own
network of DHT nodes, which can incur significant cost if such a network was not
already deployed, or piggy back off an open authenticated network, as is the
case with our proof-of-concept \DHTSYS{} implementation.

These reasons make DNS with DNSSEC more appealing as an authenticated backend in
comparison. Regardless, we provide \DHTSYS{} to demonstrate the utility and
flexibility of the \SYSTEM{} approach.

\subsubsection{Limitations of a Chrome Extension}

Our current JavaScript proof-of-concept implementations, as Chrome extensions,
are not allowed to touch the resource files downloaded by Chrome and so cannot
prevent a potentially-malicious resource from being executed by the end user---a
feature Chrome/Chromium reserves for its own internal use. The Chrome
\textit{app} API~\cite{AppAPI} might have been of assistance as it allowed for
some limited filesystem traversal via a now deprecated native app API; there is
also a non-standard HTML5/WebExtensions FileSystem API that would provide
similar functionality were it to be widely considered~\cite{deadSpec}.

While still effective, \DNSSYS{} and \DHTSYS{} would be even more effective as
browser extensions if Chrome/Chromium or the WebExtensions API allowed for an
explicit \texttt{onComplete} event hook in the downloads API. This hook would
fire immediately before a file download completed and the file became
executable, \ie had its \texttt{.crdownload} or \texttt{.download} extension
removed. The hook would consume a \texttt{Promise}/\texttt{AsyncFunction} that
kept the download in its non-complete state until said \texttt{Promise}
completed. This protocol would allow the extensions' background pages to do
something like alter the download's \texttt{DangerType} property and alert the
end user to the dangerous download naturally. These modifications would have the
advantage of communicating intent through the browser's familiar UI and
preventing the potentially-malicious download from becoming immediately
executable. Unfortunately, the closest the Chrome/WebExtensions API comes to
allowing \texttt{DangerType} mutations is the \texttt{acceptDanger} method on
the downloads API, but it is not suitable for use with \DNSSYS{} as a background
page based extension.

While nice to have, we stress that none of the aforesaid functionality is
critical to the ability of our implementations to more effectively mitigate SCA
risk than checksums and other solutions (cf. \secref{evaluation}).

\subsection{Future Work}

\subsubsection{Merkle Trees and Early Resource Validation}

Using Merkle trees~\cite{Merkle} instead of pure cryptographic hashing functions
for resource validation would enable partial verification of large files. For
example, suppose we are downloading a 10TiB resource and it is compromised. By
calculating a Merkle tree beforehand, we do not have to wait for the resource to
finish downloading before we render a failing judgment. This partial
verification has the potential to save the user a significant amount of time,
though using Merkle trees for resource integrity validation over the internet is
decidedly not-trivial~\cite{Merkle-HTTP}.

For a production example of Merkle tree based resource integrity validation, we
can look to the so-called \emph{Tiger tree hash}~\cite{TTH, Merkle} (TTH)
construction. The TTH, a Merkle tree implementation, is built on the Tiger
cryptographic hashing function. Merkle trees and TTHs are well-studied and
widely deployed constructions capable of supporting ``partial verification'' of
resources as they are downloaded. Tiger tree hashes in particular are popular
among several large P2P file sharing applications such as WireShare
(LimeWire)~\cite{LimeWire}. Of course, a solution need not be tightly coupled to
the Tiger cryptographic hashing function. The high-speed BLAKE2, SHA2, or SHA3
cryptographic hashing functions would perform just as well, if not better.

\subsubsection{URNs for Better CDN/Mirror Handling}

The goal of the Resource Identifiers (RI) is very similar to that of Uniform
Resource Names (URN). It may make sense to replace the mapping between RIs and
Authoritative Checksums with purely URN-based DNS lookups that return specially
formatted TXT records upon success. This would further simplify the deployment
process for service administrators since DNS updates would be based upon the
resource's contents instead of both its contents \textit{and where it is located
physically on a distribution system}. It may also allow for additional
confirmation methods of the identical resources in different domains and in
different locations.

We did not choose a URN-based scheme in our initial approach due to a new URN
scheme requiring the registration of a unique identifier with the Internet
Assigned Numbers Authority. Going forward, we can potentially adopt a URN scheme
that already exists, such as Magnet links~\cite{MagnetLinks} or the informal
IETF draft for hash-based URN namespaces~\cite{draft-URN}. URNs would enable
\DNSSYS{} and \DHTSYS{} to be much more resilient in how they map resources to
Authoritative Checksums, handling mirrors and CDNs without relying on a
well-known resource path as our proof-of-concept implementations currently do.
