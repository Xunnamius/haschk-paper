\section{Evaluation} \label{sec:evaluation}

In this section, we evaluate our implementation empirically using a patched
HotCRP instance and random sampling of papers published in previous
\CONFERENCE{} proceedings. We then examine the performance impact, deployment
overhead, and scalability of our implementation. We show that \SYSTEM{} is more
effective than existing approaches at detecting integrity problems in arbitrary
downloads over the internet.

\subsection{Security and Performance}

\TODO{Rewrite this subsection!}

To empirically evaluate our implementation, we launch a lightly modified version
of the popular open source research submission and peer review software,
\emph{HotCRP} (version 2.102). Our modifications allow anyone visiting the site
to interactively corrupt submissions and manipulate relevant DNS entries at
will.

For our evaluation, we upload 10 different \CONFERENCE{} PDFs to our HotCRP
instance. Upon their upload, HotCRP calculated and displayed the unique checksum
(a SHA-256 digest) of each PDF. After each PDF is uploaded, we immediately
download it and manually calculate a local checksum, matching each to the
checksum displayed by the HotCRP software. Next, we utilize the custom
functionality we added to our HotCRP instance to populate our DNS backend with
each file's current ``original'' checksum. Each checksum is considered an
Authoritative Checksum (AC) mapped to a Resource Identifier (RI) corresponding
to the uploaded PDF item.

After installing the frontend into our Google Chrome browser, we again download
each file. For each observed download, our implementation reported a ``safe''
judgement as expected. We then utilize the custom functionality we added to our
HotCRP instance to add junk data onto the end of each of the uploaded PDFs,
corrupting them. We also modified HotCRP so that it updated the displayed
checksums to match their now-corrupted counterparts.

Once again, we download each file and calculate a local checksum. Our
implementation reported an ``unsafe'' judgement (a true positive) for each
corrupted PDF file, as expected. Calculating the local checksum and checking it
against the value reported by our HotCRP instance leads to a match (a false
negative; \ie{, the result of co-location}) as expected.

We ran this experiment three times and observed consistent results.

Finally, we implement a ``redirection'' attack where, when clicking the link to
download the PDF document from HotCRP, users were forced to navigate to a
``compromised'' PHP script on an adjacent server that very quickly redirected
clients several times before triggering the download of a corrupted version of
the original HotCRP-hosted resource. Our implementation correctly flagged this
download as suspicious once the download began, successfully warning the user.

While evaluating our implementation, we observe no additional onerous network
load or CPU usage with the extension loaded into Chrome. Measurements were taken
using the Chrome developer tools. Intuitively, this makes sense since we makes
at most two queries to the backend before rendering a judgement.

To evaluate our implementation, we connect to the global Ring OpenDHT network.
Since the OpenDHT software is implemented in C++, it could not be included
directly in a JavaScript plugin. Therefore, for our implementation, we set up a
local HTTP REST server wrapping the C++ implementation of OpenDHT. Our OpenDHT
REST server provides an interface consistent with the one expected by our
implementation. See \secref{availability}, where we make these implementations
available open source for community consideration.

For our evaluation, we manually calculate a local checksum for 10 different
\CONFERENCE{} PDFs. For the OD here we use a static locally resolved domain name
corresponding to a location on a local server. We manually store these RI-to-AC
mappings as key-value pairs on the Kademlia-based OpenDHT network.

After installing the frontend into our Google Chrome browser, we download each
file from our local server. For each observed download, we reported a ``safe''
judgement as expected. We then store randomly generated checksums as replacement
RI-to-AC mappings in the Ring OpenDHT network corresponding to these 10 PDFs
such that the ACs purposely would not match the NACs generated by our
implementation, simulating file corruption by an attacker.

Once again, we download each file and calculate a local checksum. Our
implementation reported an ``unsafe'' judgement for each corrupted PDF file, as
expected.

\TODO{integrate this in here somehow: Finally, given the lightweight nature of
the cryptographic operations involved, we observed no download performance
impact compared to downloads without \SYSTEM{}.}

\TODO{One para blurb about the implications of a compromised backend, i.e.
potential denial of service. Much bigger problems if DNS is compromised,
though.}

\subsection{Deployment and Scalability}

\TODO{Argue why \SYSTEM{} is scalable and rewrite all of this mess.}

Clearly, the DNS backend relies on DNS. However, DNS~\cite{DNS1} was not
originally designed to transport or store relatively large amounts of data,
though this has been addressed with EDNS0~\cite{EDNS}. The checksums stored in
DNS should not be much longer than 128 bytes or the output of the SHA512
function. Regardless, DNS resource record extensions exist that store much more
than 128 bytes of data~\cite{CERT, IPSECKEY, DANE3, DANE1}.

Several working groups are considering DNS as a storage medium for
checksums/hash output as well, such as securitytxt~\cite{draft-sectxt}. A widely
deployed example of DNS ``TXT'' resource records being used this way is SPF and
DKIM~\cite{DKIM}. We are unaware of any practical limitation on the number of
resource records a DNS zone file can support~\cite{DNS1}, hence any
considerations regarding zone file size and/or ceilings on the number of TXT
records in a single zone are at the sole discretion of the implementing entity.

Additionally, the DNS-based backend does not add to the danger of amplification
and other reflection attacks on DNS; these are generic DNS issues addressable at
other layers of the protocol.

In respect to the DNS network, storing cryptographic data in DNS resource
records is not unprecedented. The DNS-Based Authentication of Named Entities
(DANE) specification~\cite{DANE1, DANE2, DANE3} defines the ``TLSA'' and
``OPENPGPKEY'' DNS resource records to store cryptographic data. These resource
record types, along with ``CERT''~\cite{CERT}, ``IPSECKEY''~\cite{IPSECKEY},
those defined by DNS Security Extensions (DNSSEC)~\cite{DNSSEC}, and others
demonstrate that storing useful cryptographic data retrievable through the DNS
network is feasible at scale. Due the unique requirements of DNS, however, we
use ``TXT'' records to map Resource Identifiers to Authoritative Checksums. In
accordance with RFC 5507~\cite{RFC5507}, a production implementation
implementation would necessitate the creation of a new DNS resource record type.

As \SYSTEM{} is predicated on a high availability backend and requires no
application/frontend source code changes to function, we conclude that the
scalability of \SYSTEM{} can be reduced to the scalability of its backend. We
are aware of no other obstacles to scalability beyond those inherited from the
underlying backend system.

In respect to DNS specifically, we are aware of no practical limits or
protocol-based restrictions on the scalability of a backend file itself or its
sub-zones. A service can host tens of thousands of resource records in their
backend file~\cite{DNS1, DNS2}.

With the HotCRP demo, the totality of our resource deployment scheme consisted
of the addition of a new TXT entry to our backend file---accomplished via API
call to Google DNS---during HotCRP's paper submission process. This new TXT
entry consisted of a mapping between a RI and its corresponding AC.

We find a DNS record addition or update during the resource deployment process
to be simple enough for service administrators to implement and presents no
significant burden to deployment outside of DNS API integration into a
development team or other entity's software deployment toolchain. For reference,
we implemented the functionality that automatically adds (and updates) the DNS
records mapping the ACs and RIs of papers uploaded to our HotCRP instance in
under 10 lines of JavaScript.

We note that, in the case where an entity's content distribution mechanism
relies on, for instance, a mirroring service, third-party CDN, et cetera
\emph{that randomly or disparately mangles resource URI paths}, our
implementations currently require each ``mangled'' RI permutation representing a
resource to be added to the backend, even if they all represent the same
resource by a different name/path.

\subsection{Limitations}

\TODO{Rewrite and finish me.}

Our current JavaScript implementations, as Chrome extensions, are not allowed to
touch the resource files downloaded by Chrome and so cannot prevent a
potentially-malicious resource from being executed by the user---a feature
Chrome/Chromium reserves for its own internal use. The Chrome \textit{app}
API~\cite{AppAPI} might have been of assistance as it allowed for some limited
filesystem traversal via a now deprecated native app API; there is also a
non-standard HTML5/WebExtensions FileSystem API that would provide similar
functionality were it to be widely considered~\cite{deadSpec}.

While still effective, our implementation would be even more effective as
browser extensions if Chrome/Chromium or the WebExtensions API allowed for an
explicit \texttt{onComplete} event hook in the downloads API. This hook would
fire immediately before a file download completed and the file became
executable, \ie{ had its \texttt{.crdownload} or \texttt{.download} extension
removed}. The hook would consume a \texttt{Promise}/\texttt{AsyncFunction} that
kept the download in its non-complete state until said \texttt{Promise}
completed. This would allow the extensions' background pages to do something
like alter a download's \texttt{DangerType} property and alert the user to a
dangerous download naturally. These modifications would have the advantage of
communicating intent through the browser's familiar UI and preventing the
potentially-malicious download from becoming immediately executable.
Unfortunately, the closest the Chrome/WebExtensions API comes to allowing
\texttt{DangerType} mutations is the \texttt{acceptDanger} method on the
downloads API, but it is not suitable for use with a background page based
extension.

While nice to have, we stress that none of the aforesaid functionality is
critical to the ability of our implementations to more effectively mitigate SCA
risk than checksums and other solutions (cf. \secref{evaluation}).

\TODO{Redirection, like with URI shortening services, might still break things
even with fallthrough functionality if a domain on the redirect chain has DNS
entries \SYSTEM{} recognizes}

\TODO{Does not work for PDFs and other downloads handled directly by the
browser}

\TODO{Given the topology of the webrequest API, iframes and the like may require
special consideration. May need to compare documenturi with origin uri.}

\TODO{Not all servers/backends on the internet use DNS. Direct IP not
supported.}

\TODO{Right now our PoC extension keeps 1000 requests in memory so that they can
be mapped to download items. This might be vulnerable to attacks involving
excessive redirection.}

\TODO{An ideal implementation is able to rely on Google Chrome's effective
dangerous download UI~\cite{ChromeClickThrough}. AcceptDanger does not work for
downloads that have already finished, however. Suggestion to the Chrome dev team
to implement something nicer.}
