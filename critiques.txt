Review #94A
===========================================================================

Review recommendation
---------------------
1. Reject

Overall merit
-------------
2. Top 50% but not top 25% of submitted papers

Writing quality
---------------
3. Adequate

Reviewer expertise
------------------
4. I know a lot about this area

Paper summary
-------------
This work presents and evaluates an approach to support users in mitigating malicious downloads. The given approach HASCHK replaces manual checksum-based procedures with an automatic checksum verification mechanism that does not require any user interaction (as long as nothing goes wrong) which relies on authenticated and highly available systems. The authors provide proof-of-concept implementations for Google Chrome based on DNSSEC and distributed hash tables.

Strengths
---------
- Interesting and important research question.
- Impressive amount of work went into this submission.
- Well-written and easy-to-follow.

Weaknesses
----------
- Misses critical related work.
- Does not consider transparency approaches (e.g. binary transparency).
- Uses DNSSEC for deployment.
- No human subjects experiment to evaluate usability claims.

Detailed comments for authors
-----------------------------
I want to thank the authors for the submission. The research question the authors address is a pressing issue in computer security and providing a secure and usable solution is a serious challenge. I enjoyed reading the paper. It is well-written and easy-to-follow. It impresses to see how much work the authors put into the work - in particular into the proof-of-concept implementations. However, the submission has some severe shortcomings which prevent me from recommending to accept the paper.

Sadly, the authors miss essential related work. Cherubini et al. presented the paper "Towards Usable Checksums: Automating the Integrity Verification of Web Downloads for the Masses" at CCS'18. They conduct a set of user studies to investigate the challenges of manual checksum verification and build a system for automatic integrity verification of web downloads. Finally, they perform another user study and evaluate the usability of their approach. At first glance this submission does not seem to offer a lot more than last year's CCS paper. However, to better assess the actual contribution of this, the submission's authors need to discuss the above article in their related work section.

Second, the authors do not consider all the transparency approaches (e.g., Certificate Transparency - https://www.certificate-transparency.org/ and Binary Transparency - https://wiki.mozilla.org/Security/Binary_Transparency) which have similar goals as the HASCHK approach presented in this paper. Again, the authors need to add the ideas and concept of the different transparency approach and need to illustrate HASCHK's advantages.

The authors propose to use DNSSEC to deploy HASCHK. However, DNSSEC did not find broad adoption so far and did not seem to work well as a piggy-back infrastructure for any new security mechanism in the past.

In the course of the paper, the authors mention the importance of good usability for a novel security mechanism. Sadly, the authors do not provide any usability evaluation of their tool. Specifically in comparison to the work by Cherubini et al., a user study seems to be crucial for a system described in the submission.

Overall, I encourage the authors to continue this line of work. There are a lot of open questions in this space. However, I recommend them to integrate their research into related work better.


* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *


Review #94B
===========================================================================

Review recommendation
---------------------
1. Reject

Overall merit
-------------
1. Bottom 50% of submitted papers

Writing quality
---------------
4. Well-written

Reviewer expertise
------------------
4. I know a lot about this area

Paper summary
-------------
This paper proposes HASCHK, a browser extension for checking the hash of a submitted resource against an authoritative version pulled from DNSSEC.

Strengths
---------
+Detailed implementation notes

Weaknesses
----------
-Technical novelty is minimal

-Users will not go for this approach

-Users will be confused

Detailed comments for authors
-----------------------------
This paper is written up very clearly and in a lot of detail, but I think the fundamental idea is (a) not very new and (b) unlikely to work in practice with real users.

There are a lot of ideas for checking the integrity of a download, including sub-resource integrity as discussed here and also Stickler (Levy et al.), an important missing reference, which is similar to SRI in some ways but more powerful.

From what I can tell what is new here is that DNSSEC is used to retrieve the correct hash for a resource. There are a few practical problems with this (incomplete DNSSEC deployment, etc.) but the paper does a fair job presenting those. I also wonder about the assumption that in practice that an attacker who can compromise a web serve can't also compromise DNSSEC in some cases.

There is still a massive flaw here: an attacker who can compromise the webserver can simply present a different link to a different domain hosting whatever content they want, which will avoid the DNSSEC checks (or direct the critical resource download to a URL they can control DNSSEC for) and thus defeat this mechanism. Detecting this would require users to manually check which URL they are downloading the resource (e.g. Linux image) from and have some clue about if it was correct or not.

In practice, users get hyperlinks from websites, to the extent that they can authenticate anything it's the URL (as indicated by the padlock icon and standard web PKI UI), an attacker who can undermine this can easily evade the DNSSEC check.

Another flaw here is that users can "override" an incorrect hash in DNSSEC. I suspect most users will do this in practice.

There were some nice pieces to this paper, like the case history in section 2.2. But overall this paper could be four pages long and present the actual novel idea here. The performance section is pretty superfluous, the crypto operations are so obviously lightweight there's no real doubt it would have negligible performance impact. The paper repeats itself a lot in setting up the threat model and discussing related work (Section 2.3 could be two sentences). I'd encourage the authors to read the Stickler paper-that was a workshop paper, 8 pages long, presented the exact same threat model here much more succinctly as well as more technical content.

In any case though I don't think there's enough new here to publish this as a major research contribution unless a user study somehow showed that this mechanism works in practice

Questions for authorsâ€™ response
---------------------------------
Please challenge or correct my thinking: Why can't an attacker who compromises a web server evade this completely?
