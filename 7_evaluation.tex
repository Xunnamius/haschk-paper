\section{Evaluation} \label{sec:evaluation}

The primary goal of any \SYSTEM{} implementation is to alert end-users when the
resource they have downloaded is something other than what they were expecting.
In this section, we evaluate our approach by first assessing the threat model
\SYSTEM{} addresses, followed by an examination of our Google Chrome extensions,
\DNSSYS{} and \DHTSYS{}. We then test our implementations versus a real-world
deployment of HotCRP and/or a random sampling of papers published in previous
\CONFERENCE{} proceedings. Finally, we evaluate the obstacles to scalability and
potential performance overhead of our extensions.

\TODO{Add metrics about solution LoC}

To evaluate the effectiveness of our mitigation, we test \DNSSYS{} and
\DHTSYS{}---our \SYSTEM{} Chrome extension implementations---against a series of
common real-world resource integrity violations. The impetus behind any such
resource integrity SCA is to have the resource pass through undetected by
abusing the trust between client and provider with the hope that an unsuspecting
user will interact with it.

We show that the \SYSTEM{} approach is more effective than existing approaches
at detecting integrity violations in arbitrary resources on the internet; this
is especially evident when \DNSSYS{} and \DHTSYS{} are compared to the de facto
standard: checksums.

\subsection{\DNSSYS{}}

To empirically evaluate \DNSSYS{}, we launch a heavily modified version of the
popular open source research submission and peer review software, \emph{HotCRP}
(version 2.102). Our modifications allow anyone visiting the site to
interactively corrupt submissions and manipulate relevant DNS entries at will.

For our evaluation, we upload 10 different \CONFERENCE{} PDFs to our HotCRP
instance. Upon their upload, HotCRP calculated and displayed the unique checksum
(a SHA-256 digest) of each PDF. After each PDF is uploaded, we immediately
download it and manually calculate a local checksum, matching each to the
checksum displayed by the HotCRP software. Next, we utilize the custom
functionality we added to our HotCRP instance to populate our DNS backend with
each file's current "original" checksum. Each checksum is considered an
Authoritative Checksum (AC) mapped to a Resource Identifier (RI) corresponding
to the uploaded PDF item.

After installing \DNSSYS{} into our Google Chrome browser, we again download
each file. For each observed download, \DNSSYS{} reported a ``safe'' judgement
as expected. We then utilize the custom functionality we added to our HotCRP
instance to add junk data onto the end of each of the uploaded PDFs, corrupting
them. We also modified HotCRP so that it updated the displayed checksums to
match their now-corrupted counterparts.

Once again, we download each file and calculate a local checksum. \DNSSYS{}
reported an ``unsafe'' judgement (a true positive) for each corrupted PDF file,
as expected. Calculating the local checksum and checking it against the value
reported by our HotCRP instance leads to a match (a false negative; \ie the
result of co-location) as expected.

We ran this experiment three times and observed consistent results.

Finally, we implement a ``redirection'' attack where, when clicking the link to
download the PDF document from HotCRP, users were forced to navigate to a
``compromised'' PHP script on an adjacent server that very quickly redirected
clients several times before triggering the download of a corrupted version of
the original HotCRP-hosted resource. Our implementation correctly flagged this
download as suspicious once the download began, successfully warning the user.

While evaluating our implementation, we observe no additional onerous network
load or CPU usage with the extension loaded into Chrome. Measurements were taken
using the Chrome developer tools. Intuitively, this makes sense since \DNSSYS{}
makes at most two queries to the backend before rendering a judgement.

\subsection{\DHTSYS{}}

To evaluate \DHTSYS{}, we connect to the authenticated global Ring OpenDHT
network. Since the OpenDHT software is implemented in C++, it could not be
included directly in a JavaScript plugin. Therefore, for our implementation, we
set up a local HTTP REST server wrapping the C++ implementation of OpenDHT. Our
OpenDHT REST server provides an interface consistent with the one expected by
\DNSSYS{} (\ie Google DNS's REST API), allowing for code reuse (\eg redirection
protection) between our two implementations. See \secref{availability}, where we
make these implementations available open source for community consideration.

For our evaluation, we manually calculate a local checksum (\ie an AC) and an RI
for 10 different \CONFERENCE{} PDFs. For the OD here we use a static locally
resolved domain name corresponding to a location on a local server. We manually
store these RI-to-AC mappings as key-value pairs on the Kademlia-based OpenDHT
network.

After installing \DHTSYS{} into our Google Chrome browser, we download each file
from our local server. For each observed download, \DHTSYS{} reported a ``safe''
judgement as expected. We then store randomly generated checksums as replacement
RI-to-AC mappings in the Ring OpenDHT network corresponding to these 10 PDFs
such that the ACs purposely would not match the NACs generated by \DHTSYS{},
simulating file corruption by an attacker.

Once again, we download each file and calculate a local checksum. \DHTSYS{}
reported an ``unsafe'' judgement for each corrupted PDF file, as expected.

\subsection{Obstacles to Scalability, Deployment}

As \SYSTEM{} is predicated on a distributed authenticated highly-available
backend and requires no application/frontend source code changes to function, we
conclude that the scalability of \SYSTEM{} can be reduced to the scalability of
its backend. We are aware of no other obstacles to scalability beyond those
inherited from the underlying backend system.

In respect to DNS specifically, we are aware of no practical limits or
protocol-based restrictions on the scalability of a backend file itself or its
sub-zones. A service can host tens of thousands of resource records in their
backend file~\cite{DNS1, DNS2}.

With the HotCRP demo, the totality of our resource deployment scheme consisted
of the addition of a new TXT entry to our backend file---accomplished via API
call to Google DNS---during HotCRP's paper submission process. This new TXT
entry consisted of a mapping between a RI and its corresponding AC.

We find a DNS record addition or update during the resource deployment process
to be simple enough for service administrators to implement and presents no
significant burden to deployment outside of DNS API integration into a
development team or other entity's software deployment toolchain. For reference,
we implemented the functionality that automatically adds (and updates) the DNS
records mapping the ACs and RIs of papers uploaded to our HotCRP instance in
under 10 lines of JavaScript.

We note that, in the case where an entity's content distribution mechanism
relies on, for instance, a mirroring service, third-party CDN, et cetera
\emph{that randomly or disparately mangles resource URL paths}, our
implementations currently require each "mangled" RI permutation representing a
resource to be added to the backend, even if they all represent the same
resource by a different name/path. This issue and its solutions are discussed
further in the context of URNs in \secref{discussion}.
